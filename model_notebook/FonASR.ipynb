{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FonASR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibD6bsRPl8Qu"
      },
      "source": [
        "Implementation of Fon Automatic Speech Recognition model, from original paper: *«OkwuGbé: End-to-End Speech Recognition for Fon and Igbo»*, *Accepted at African NLP, EACL 2021.*\r\n",
        "\r\n",
        "*Authors: Bonaventure F. P. Dossou, and Chris C. Emezue*\r\n",
        "\r\n",
        "***Addition: Attention Mechanism added by the Bonaventure F. P. Dossou***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rdj561RcLMd"
      },
      "source": [
        "!pip install torchaudio==0.4.0 torch==1.4.0 > /dev/null\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "import os\r\n",
        "from IPython.display import Audio, display\r\n",
        "from scipy.io import wavfile\r\n",
        "import unicodedata\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.utils.data as data\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchaudio\r\n",
        "import numpy as np\r\n",
        "import zipfile\r\n",
        "import csv\r\n",
        "import random\r\n",
        "import os\r\n",
        "from typing import Tuple, Optional\r\n",
        "import csv\r\n",
        "import random\r\n",
        "import tensorflow as tf\r\n",
        "from torch import Tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIjWwSp2LwPV"
      },
      "source": [
        " # Download the files from Dr. Layele's Github to the directory FonAudio\r\n",
        "if not os.path.isdir(\"./FonAudio\"):\r\n",
        "  !wget https://github.com/laleye/pyFongbe/archive/master/data.zip\r\n",
        "  with zipfile.ZipFile(\"data.zip\",\"r\") as zip_ref:\r\n",
        "    zip_ref.extractall(\"./FonAudio\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4p8XxzbU9K0"
      },
      "source": [
        "# Get valid indices\r\n",
        "random.seed(42)\r\n",
        "with open('./FonAudio/pyFongbe-master/data/test.csv', newline='',encoding='UTF-8') as f:\r\n",
        "      reader = csv.reader(f)\r\n",
        "      data = list(reader)\r\n",
        "      data = [data[i] for i in range(len(data)) if i!=0]\r\n",
        "\r\n",
        "v = 1500 # number of validation speech samples\r\n",
        "test_list = [i for i in range(len(data))]\r\n",
        "valid_indices = random.choices(test_list, k=v)\r\n",
        "BATCH_MULTIPLIER = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnMlym3vMqCQ"
      },
      "source": [
        "\r\n",
        "def get_data(datatype): # can be either train or test. Any other format will throw an error.\r\n",
        "  if datatype != \"valid\":\r\n",
        "    de=datatype\r\n",
        "  else:\r\n",
        "    de = \"test\"\r\n",
        "\r\n",
        "  with open('./FonAudio/pyFongbe-master/data/{}.csv'.format(de), newline='',encoding='UTF-8') as f:\r\n",
        "      reader = csv.reader(f)\r\n",
        "      data = list(reader)\r\n",
        "      data = [data[i] for i in range(len(data)) if i!=0]\r\n",
        "\r\n",
        "  if datatype == \"test\":\r\n",
        "    test_data = [data[i] for i in range(len(data)) if i not in valid_indices]\r\n",
        "    return test_data\r\n",
        "\r\n",
        "  if datatype == \"valid\": # then we should get out some for valid\r\n",
        "    val_data = [data[i] for i in valid_indices]\r\n",
        "    return val_data\r\n",
        "\r\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN14HWAaXh-G"
      },
      "source": [
        "import torchaudio\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchaudio.datasets.utils import (\r\n",
        "    download_url,\r\n",
        "    extract_archive,\r\n",
        "    walk_files,\r\n",
        ")\r\n",
        "\r\n",
        "def load_audio_item(d: list):\r\n",
        "    utterance = d[2]\r\n",
        "    wav_path = d[0]\r\n",
        "    wav_path = wav_path.replace(\"/home/frejus/Projects/Fongbe_ASR/pyFongbe\",\"./FonAudio/pyFongbe-master\")\r\n",
        "    print(wav_path)\r\n",
        "    # Load audio\r\n",
        "    waveform, sample_rate = torchaudio.load(wav_path)\r\n",
        "    \r\n",
        "    return (waveform, \r\n",
        "        sample_rate,\r\n",
        "        utterance\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "class FonASR(torch.utils.data.Dataset):\r\n",
        "    \"\"\"Create a Dataset for Fon ASR.\r\n",
        "    Args:\r\n",
        "    data_type could be either 'test', 'train' or 'valid'\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, data_type):\r\n",
        "\r\n",
        "      \"\"\"data_type could be either 'test', 'train' or 'valid' \"\"\"\r\n",
        "      self.data = get_data(data_type)\r\n",
        "\r\n",
        "    def __getitem__(self, n: int):\r\n",
        "      \"\"\"Load the n-th sample from the dataset.\r\n",
        "\r\n",
        "      Args:\r\n",
        "          n (int): The index of the sample to be loaded\r\n",
        "\r\n",
        "      Returns:\r\n",
        "          tuple: ``(waveform, sample_rate, utterance)``\r\n",
        "      \"\"\"\r\n",
        "      fileid = self.data[n]\r\n",
        "      return load_audio_item(fileid)\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "      return len(self.data)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1fXgsDQmK09"
      },
      "source": [
        "## installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWU5n4f8DWUO"
      },
      "source": [
        "accent_code = [b'\\\\u0301',b'\\\\u0300',b'\\\\u0306',b'\\\\u0308',b'\\\\u0303']\r\n",
        "alpha = {'ɔ':0,'ɛ':5}\r\n",
        "accents = {b'\\\\u0301':1,b'\\\\u0300':2,b'\\\\u0306':3,b'\\\\u0308':4,b'\\\\u0303':5}\r\n",
        "mapping={\r\n",
        "    1:'ɔ́',2:'ɔ̀',3:'ɔ̆',6:'έ',7:'ὲ',8:'ɛ̆'\r\n",
        "}\r\n",
        "#we are following the idea that the composition gives the letter first followed by the sign(accent)\r\n",
        "def get_better_mapping(text):\r\n",
        "  t_arr = [t for t in text]\r\n",
        "  s=[]\r\n",
        "  for i in range(len(t_arr)):\r\n",
        "    if t_arr[i].encode(\"unicode_escape\") in accent_code:\r\n",
        "      to_check = s[-1]\r\n",
        "      try:\r\n",
        "        val = mapping[alpha[to_check] + accents[t_arr[i].encode(\"unicode_escape\")]]\r\n",
        "        s.pop()\r\n",
        "        s.append(val)\r\n",
        "      except KeyError:\r\n",
        "        print(\"Could not find for {} in sentence {} | Proceeding with default.\".format(t_arr[i],text))\r\n",
        "      \r\n",
        "    else: \r\n",
        "      s.append(t_arr[i])\r\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSKHvy8DmOCQ"
      },
      "source": [
        "## Setting up your data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVJs4Bk8FjjO"
      },
      "source": [
        "\n",
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
        "    between two sequences. Informally, the levenshtein disctance is defined as\n",
        "    the minimum number of single-character edits (substitutions, insertions or\n",
        "    deletions) required to change one word into the other. We can naturally\n",
        "    extend the edits to word level when calculate levenshtein disctance for\n",
        "    two sentences.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in word-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Levenshtein distance and word number of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in char-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Levenshtein distance and length of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
        "    hypothesis text in word-level. WER is defined as:\n",
        "    .. math::\n",
        "        WER = (Sw + Dw + Iw) / Nw\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sw is the number of words subsituted,\n",
        "        Dw is the number of words deleted,\n",
        "        Iw is the number of words inserted,\n",
        "        Nw is the number of words in the reference\n",
        "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
        "    that empty items will be removed when splitting sentences by delimiter.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Word error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If word number of reference is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
        "    hypothesis text in char-level. CER is defined as:\n",
        "    .. math::\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
        "    encoded to unicode. Please draw an attention that the leading and tailing\n",
        "    space characters will be truncated and multiple consecutive space\n",
        "    characters in a sentence will be replaced by one space character.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Character error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If the reference length is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        r 18\n",
        "        s 19\n",
        "        t 20\n",
        "        u 21\n",
        "        v 22\n",
        "        w 23\n",
        "        x 24\n",
        "        y 25\n",
        "        z 26\n",
        "        à 27\n",
        "        á 28\n",
        "        è 29\n",
        "        é 30\n",
        "        ì 31\n",
        "        í 32\n",
        "        î 33\n",
        "        ï 34\n",
        "        ó 35\n",
        "        ù 36\n",
        "        ú 37\n",
        "        ā 38\n",
        "        ă 39\n",
        "        ē 40\n",
        "        ĕ 41\n",
        "        ŏ 42\n",
        "        ū 43\n",
        "        ŭ 44\n",
        "        ɔ 45\n",
        "        ɖ 46\n",
        "        ò 47\n",
        "        ε 48\n",
        "        έ 49\n",
        "        ɔ̀ 50\n",
        "        ɔ̆ 51\n",
        "        ὲ 52\n",
        "        ɔ́ 53\n",
        "        ĭ 54\n",
        "        ɛ̆ 55\n",
        "        ɛ̃ 56\n",
        "        . 57\n",
        "        , 58\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        text=unicodedata.normalize(\"NFC\",text)\n",
        "        for c in get_better_mapping(text):\n",
        "            try:\n",
        "              if c == ' ':\n",
        "                  ch = self.char_map['<SPACE>']\n",
        "              elif c =='̀':\n",
        "                  ch=0\n",
        "              else:\n",
        "                  ch = self.char_map[c]\n",
        "            except KeyError:\n",
        "              print(\"Error for character {} in this sentence: {}\".format(c,text))\n",
        "              ch=0\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "test_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "\n",
        "    for waveform,_,utterance in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'test':\n",
        "            spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train, valid or test')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "    \n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=59, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j-1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XdSlhAQnDEA"
      },
      "source": [
        "## The Model\n",
        "Base of of Deep Speech 2 with some improvements including the addition of the attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65H1-PCjm-FB"
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, hidden = self.BiGRU(x)\n",
        "        \n",
        "        # Beginning of the attention part  \n",
        "        if self.batch_first:\n",
        "          hidden = hidden.transpose(0, 1).contiguous()\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        w1 = nn.Linear(x.size()[2], x.size()[2]//2).to(device)\n",
        "        w2 = nn.Linear(hidden.size()[2], hidden.size()[2]).to(device)\n",
        "        \n",
        "        v = nn.Linear(x.size()[2]//2, x.size()[2]).to(device)\n",
        "\n",
        "        x1 = w1(x).to(device)\n",
        "        hidden2 = w2(hidden).to(device)\n",
        "        \n",
        "        del w1, w2\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # atrick to make size match\n",
        "        # we add \"0\"s so that the addition with x1 doesn't change in principle anything\n",
        "        # and tanh(x|x=0) = 0\n",
        "        if hidden.size()[1] != x1.size()[1]:\n",
        "          additional = np.full((hidden.size()[0], x1.size()[1]-hidden.size()[1],hidden.size()[2]), 0)\n",
        "          hidden2 = torch.cat((hidden2, torch.tensor(additional, dtype=torch.float).to(device)), 1)\n",
        "          del additional\n",
        "          torch.cuda.empty_cache()\n",
        "        \n",
        "        m = nn.Tanh()\n",
        "        score = v(m(x1+hidden2)).to(device) # compute attention scores\n",
        "\n",
        "        del x1, hidden2, m, v\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        n = nn.Softmax()\n",
        "        attention_weights = n(score) # get attention weights\n",
        "\n",
        "        del score, n\n",
        "        torch.cuda.empty_cache()\n",
        "        context_vector = attention_weights * x # compute the attention vector\n",
        "        x = torch.cat((context_vector, x), axis=-1) # apply context vector to the input\n",
        "\n",
        "        del context_vector\n",
        "        torch.cuda.empty_cache()\n",
        "        # End of the attention part\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "\n",
        "        self.BiLSTM = nn.LSTM(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiLSTM(x) # enc_output, (hidden_state, cell_state)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        \n",
        "        self.birnn_layers1 = nn.Sequential(*[\n",
        "            BidirectionalLSTM(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "\n",
        "        self.birnn_layers2 = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "\n",
        "        self.birnn_layers2_attention = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim*2 if i==0 else rnn_dim*4,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=True)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "    \n",
        "        self.classifier_attention = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*4, rnn_dim),  # birnn returns rnn_dim*4\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "        x = self.birnn_layers1(x).to(device)        \n",
        "        x = self.birnn_layers2_attention(x).to(device)\n",
        "        x = self.classifier_attention(x)\n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuguNEzKnMOn"
      },
      "source": [
        "## The Training and Evaluating Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydkqGeOwnPGY"
      },
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment,valid_loader,best_wer, cnn_lay, rnn_lay, model_path, rnn_dim):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    train_loss=0\n",
        "    batch_train_loss=0\n",
        "    \n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        train_loss += loss.item() / len(train_loader)\n",
        "        # used if grad accumulation is used\n",
        "        # train_loss += loss.item() / (len(train_loader)*BATCH_MULTIPLIER)\n",
        "        loss.backward()\n",
        "\n",
        "        # This is the idea of grad accumulation to overcome memory issue\n",
        "        # if (batch_idx + 1) % BATCH_MULTIPLIER == 0:\n",
        "        #     optimizer.step()\n",
        "        #     scheduler.step()\n",
        "        #     iter_meter.step()\n",
        "        #     #model.zero_grad() #reset gradients\n",
        "        #     optimizer.zero_grad()\n",
        "        #     batch_train_loss+=train_loss\n",
        "        #     train_loss=0\n",
        "        #     # print('here after gradient')\n",
        "       \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        iter_meter.step()\n",
        "\n",
        "        if batch_idx % 1000 == 0 or batch_idx == data_len:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(spectrograms), data_len,\n",
        "                      100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    experiment['loss'].append((train_loss,iter_meter.get()))\n",
        "    val_wer = valid(model, device, valid_loader, criterion, epoch, iter_meter, experiment) # wer\n",
        "    if val_wer < best_wer:\n",
        "      best_wer = val_wer\n",
        "      torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'val_loss':val_wer\n",
        "              }, model_path)\n",
        "\n",
        "    else:\n",
        "      print(\"...No improvement in validation according to WER...\")\n",
        "    return best_wer\n",
        "\n",
        "def valid(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment['val_loss'].append((test_loss, iter_meter.get()))\n",
        "    experiment['cer'].append((avg_cer, iter_meter.get()))\n",
        "    experiment['wer'].append((avg_wer, iter_meter.get()))\n",
        "\n",
        "    print('Valid set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "    return avg_wer\n",
        "    \n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment, testing_wav):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            \n",
        "            for j in range(len(decoded_preds)):\n",
        "                wav_path = testing_wav[j][0]\n",
        "                wav_path = wav_path.replace(\"/home/frejus/Projects/Fongbe_ASR/pyFongbe\",\"./FonAudio/pyFongbe-master\")\n",
        "                rate, data = wavfile.read(wav_path)\n",
        "                audio = Audio(data, rate=rate)\n",
        "                display(audio)\n",
        "                print(\"Decoding Speech's Content\")\n",
        "                print(\"Audio's Transcription: {}\".format(decoded_preds[j]))\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "                current_prediction = \"Decoded target: {}\\nDecoded prediction: {}\\n\".format(decoded_targets[j], decoded_preds[j])\n",
        "                all_predictions.append(current_prediction)\n",
        "\n",
        "    def save_list(lines, filename):\n",
        "      data = '\\n'.join(lines)\n",
        "      file = open(filename, 'w', encoding=\"utf-8\")\n",
        "      file.write(data)\n",
        "      file.close()\n",
        "\n",
        "    save_list(all_predictions, \"best_model_predictions_1.txt\")\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    \n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "def main(learning_rate, batch_size, epochs,experiment, cnn_layer, rnn_layer, model_path, rnn_dim, disabled=True):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": cnn_layer,\n",
        "        \"n_rnn_layers\": rnn_layer,\n",
        "        \"rnn_dim\": rnn_dim,\n",
        "        \"n_class\": 60,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(\"DEVICE: {}\".format(device))\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        print(\"Making dir of /data\")\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    \n",
        "    train_dataset = FonASR(\"train\")\n",
        "    valid_dataset = FonASR(\"valid\")\n",
        "    test_dataset = FonASR(\"test\")\n",
        "    \n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'test'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print(\"Model with CNN Layers == {} and RNN Layers == {}: and Rnn_Dim == {}\\n\\n\".format(cnn_layer, rnn_layer, rnn_dim))\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device) # needs to be set to 59 for further words\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    iter_meter = IterMeter()\n",
        "    best_wer= 1000\n",
        "    \n",
        "    if os.path.exists(model_path):\n",
        "      checkpoint = torch.load(model_path, map_location='cpu')\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      epoch_saved = checkpoint['epoch']\n",
        "      best_wer = checkpoint['val_loss']\n",
        "      epoch_saved = epochs\n",
        "      for epoch in range(epoch_saved+1, epochs + 1):\n",
        "        print(\"Epoch Retrieved: {} with WER: {}\".format(epoch_saved, best_wer))\n",
        "        best_wer = train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment,valid_loader,best_wer, cnn_layer, rnn_layer, model_path, rnn_dim)\n",
        "    else:      \n",
        "      for epoch in range(1, epochs + 1):\n",
        "          best_wer = train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment,valid_loader,best_wer, cnn_layer, rnn_layer, model_path, rnn_dim)    \n",
        "    print(\"Evaluating on Test data:\")\n",
        "    wav_test = get_data(\"test\")\n",
        "    test(model, device, test_loader, criterion, epochs, iter_meter, experiment, wav_test)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxRIb_WempDq"
      },
      "source": [
        "## GPU runtime\n",
        "If you are using a GPU runtime, this will let you know what GPU and how much memory is available. Adjust your batch_size depending on which GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlUSuAJwlzo8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXvlWZeVpXfX"
      },
      "source": [
        "## Train\n",
        "this will download the data on first run and may take a while. \n",
        "\n",
        "If you have Comet.ml setup, you can start seeing your progress in the comet cell above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XZodve8PGKfS"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import numpy as np\n",
        "import os\n",
        "model_path_ = '/content/drive/MyDrive/FonASR' # change accordingly to the model's folder location\n",
        "if not os.path.isdir(model_path_):\n",
        "  os.makedirs(model_path_)\n",
        "model_path_ = '/content/drive/MyDrive/FonASR/fonasr' # change accordingly to the model's base name\n",
        "\n",
        "def save_list(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w', encoding=\"utf-8\")\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "learning_rate = 5e-4\n",
        "batch_size = 20\n",
        "epochs = 500\n",
        "cnn_rnn_layers = [(5, 3, 512)]\n",
        "for cnn_rnn in cnn_rnn_layers:\n",
        "  model_path = model_path_+\"_{}_{}_{}_gru_lstm_attention\".format(cnn_rnn[0], cnn_rnn[1], cnn_rnn[2],epochs)\n",
        "  experiment={\n",
        "    'loss':[],\n",
        "    'val_loss':[],\n",
        "    'cer':[],\n",
        "    'wer':[]}\n",
        "  main(learning_rate, batch_size, epochs, experiment, cnn_rnn[0], cnn_rnn[1], model_path, cnn_rnn[2])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
